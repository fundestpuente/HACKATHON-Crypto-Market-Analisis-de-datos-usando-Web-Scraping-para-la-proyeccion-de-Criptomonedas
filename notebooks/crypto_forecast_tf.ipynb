{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a48f7a7f",
      "metadata": {},
      "source": [
        "# Crypto Forecast (Colab-ready)\n",
        "\n",
        "Conv1D + stacked LSTM forecaster on log-returns. Pulls data from a live API (`API_BASE_URL`, e.g., ngrok) or an uploaded CSV. Designed for sparse crypto snapshots: optional source filter, dynamic window shrink, small window defaults, LR scheduling, regularization, and clean exits when data is too small."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc92a6d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install --upgrade requests pandas numpy matplotlib seaborn plotly tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2715b46a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- Config (override via env) ----\n",
        "API_BASE_URL = os.getenv(\"API_BASE_URL\", \"http://localhost:8000\")\n",
        "CSV_PATH = os.getenv(\"CSV_PATH\", \"/content/market_dataset.csv\")\n",
        "# Focused symbols: set via env; default BTC (set to DOGE for the second run)\n",
        "TARGET_SYMBOL = os.getenv(\"TARGET_SYMBOL\", \"BTC\").upper()\n",
        "TARGET_SOURCE = os.getenv(\"TARGET_SOURCE\", \"CoinPaprika\")  # filter to one source for less noise\n",
        "# Small defaults for sparse data; adjust upward if you have more history\n",
        "WINDOW_SIZE = int(os.getenv(\"WINDOW_SIZE\", 12))\n",
        "HORIZON = int(os.getenv(\"HORIZON\", 1))\n",
        "EPOCHS = int(os.getenv(\"EPOCHS\", 30))\n",
        "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 32))\n",
        "VAL_SPLIT = 0.15\n",
        "TEST_SPLIT = 0.15\n",
        "def load_dataset() -> pd.DataFrame:\n",
        "    \"\"\"Attempt API fetch; fallback to uploaded CSV.\"\"\"\n",
        "    try:\n",
        "        resp = requests.get(f\"{API_BASE_URL}/markets\", params={\"limit\": 1000}, timeout=15)\n",
        "        resp.raise_for_status()\n",
        "        payload = resp.json()\n",
        "        df_api = pd.DataFrame(payload)\n",
        "        if not df_api.empty:\n",
        "            df_api[\"as_of\"] = pd.to_datetime(df_api[\"as_of\"])\n",
        "            return df_api.sort_values(\"as_of\")\n",
        "    except Exception as exc:\n",
        "        print(f\"API fetch failed, will try CSV: {exc}\")\n",
        "\n",
        "    if os.path.exists(CSV_PATH):\n",
        "        df_csv = pd.read_csv(CSV_PATH, parse_dates=[\"as_of\"])\n",
        "        return df_csv.sort_values(\"as_of\")\n",
        "\n",
        "    raise SystemExit(\"No data found. Export CSV locally or ensure the API is reachable.\")\n",
        "\n",
        "\n",
        "df = load_dataset()\n",
        "\n",
        "# Optional: filter to a single source to reduce cross-source noise\n",
        "if TARGET_SOURCE:\n",
        "    df = df[df[\"source\"] == TARGET_SOURCE]\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de56673",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter symbol and compute log returns\n",
        "df_sym = df[df[\"symbol\"].str.upper() == TARGET_SYMBOL].copy()\n",
        "df_sym = df_sym.sort_values(\"as_of\")\n",
        "if df_sym.empty:\n",
        "    raise SystemExit(f\"Symbol {TARGET_SYMBOL} not found in dataset (after source filter: {TARGET_SOURCE}).\")\n",
        "\n",
        "df_sym[\"log_return\"] = np.log(df_sym[\"price\"].astype(float)).diff()\n",
        "df_sym = df_sym.dropna(subset=[\"log_return\"])\n",
        "returns = df_sym[\"log_return\"].to_numpy()\n",
        "\n",
        "# Adjust window dynamically if data is sparse\n",
        "min_needed = WINDOW_SIZE + HORIZON + 1\n",
        "if len(returns) < min_needed:\n",
        "    suggested = max(4, len(returns) - HORIZON - 1)\n",
        "    if suggested <= 0:\n",
        "        raise SystemExit(\n",
        "            f\"Not enough data to build any window. Have {len(returns)} points; \"\n",
        "            f\"need at least {HORIZON + 2}. Run more ingests or use CSV with more history.\"\n",
        "        )\n",
        "    print(f\"Data is sparse. Reducing WINDOW_SIZE from {WINDOW_SIZE} to {suggested}.\")\n",
        "    WINDOW_SIZE = suggested\n",
        "\n",
        "def make_windows(series: np.ndarray, window: int, horizon: int):\n",
        "    xs, ys = [], []\n",
        "    for idx in range(len(series) - window - horizon + 1):\n",
        "        xs.append(series[idx : idx + window])\n",
        "        ys.append(series[idx + window : idx + window + horizon])\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "X, y = make_windows(returns, WINDOW_SIZE, HORIZON)\n",
        "X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "n = len(X)\n",
        "if n == 0:\n",
        "    raise SystemExit(\n",
        "        f\"Still no windows after adjusting window size. Have {len(returns)} return points. \"\n",
        "        \"Run more ingests or lower WINDOW_SIZE further.\"\n",
        "    )\n",
        "\n",
        "test_n = int(n * TEST_SPLIT)\n",
        "val_n = int(n * VAL_SPLIT)\n",
        "train_n = n - val_n - test_n\n",
        "if train_n < 1:\n",
        "    train_n = 1\n",
        "    val_n = max(0, n - train_n - test_n)\n",
        "    test_n = max(0, n - train_n - val_n)\n",
        "\n",
        "X_train, y_train = X[:train_n], y[:train_n]\n",
        "X_val, y_val = X[train_n : train_n + val_n], y[train_n : train_n + val_n]\n",
        "X_test, y_test = X[train_n + val_n :], y[train_n + val_n :]\n",
        "\n",
        "# Scale using training stats\n",
        "MU = X_train.mean()\n",
        "SIGMA = X_train.std() + 1e-8\n",
        "X_train = (X_train - MU) / SIGMA\n",
        "X_val = (X_val - MU) / SIGMA\n",
        "X_test = (X_test - MU) / SIGMA\n",
        "\n",
        "print(f\"Train windows: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4afdff20",
      "metadata": {},
      "outputs": [],
      "source": [
        "tf.random.set_seed(7)\n",
        "\n",
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Input(shape=(WINDOW_SIZE, 1)),\n",
        "        tf.keras.layers.Conv1D(\n",
        "            filters=32,\n",
        "            kernel_size=3,\n",
        "            padding=\"causal\",\n",
        "            activation=\"relu\",\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(1e-5),\n",
        "        ),\n",
        "        tf.keras.layers.SpatialDropout1D(0.1),\n",
        "        tf.keras.layers.LSTM(64, return_sequences=True, kernel_regularizer=tf.keras.regularizers.l2(1e-5)),\n",
        "        tf.keras.layers.Dropout(0.15),\n",
        "        tf.keras.layers.LSTM(32, kernel_regularizer=tf.keras.regularizers.l2(1e-5)),\n",
        "        tf.keras.layers.Dense(16, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-5)),\n",
        "        tf.keras.layers.Dense(HORIZON),\n",
        "    ]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
        "model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, min_delta=1e-5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5, min_lr=1e-5, verbose=1),\n",
        "]\n",
        "\n",
        "val_data = (X_val, y_val) if len(X_val) > 0 else None\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=val_data,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=2,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "test_metrics = None\n",
        "if len(X_test) > 0:\n",
        "    test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test MAE: {test_mae:.6f}\")\n",
        "    test_metrics = {\"loss\": float(test_loss), \"mae\": float(test_mae)}\n",
        "else:\n",
        "    print(\"No test set available (dataset too small); skipping test evaluation.\")\n",
        "\n",
        "model_path = f\"/content/{TARGET_SYMBOL}_model.keras\"\n",
        "meta_path = f\"/content/{TARGET_SYMBOL}_meta.json\"\n",
        "\n",
        "model.save(model_path)\n",
        "meta = {\n",
        "    \"symbol\": TARGET_SYMBOL,\n",
        "    \"window\": WINDOW_SIZE,\n",
        "    \"horizon\": HORIZON,\n",
        "    \"mu\": float(MU),\n",
        "    \"sigma\": float(SIGMA),\n",
        "    \"test\": test_metrics,\n",
        "    \"source\": TARGET_SOURCE,\n",
        "}\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(f\"Saved model to {model_path}\")\n",
        "print(f\"Saved metadata to {meta_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(X_test) == 0:\n",
        "    print(\"No test set available; skipping prediction plot.\")\n",
        "else:\n",
        "    preds = model.predict(X_test, verbose=0).squeeze()\n",
        "    truth = y_test.squeeze()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(truth, label=\"Actual\")\n",
        "    plt.plot(preds, label=\"Predicted\")\n",
        "    plt.title(f\"{TARGET_SYMBOL} returns \u2014 horizon {HORIZON}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}